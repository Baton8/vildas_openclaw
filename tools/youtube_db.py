#!/usr/bin/env python3
"""
YouTubeå‹•ç”»è§£æ â†’ DBä¿å­˜ãƒ„ãƒ¼ãƒ«

YouTube URLã‚’æ¸¡ã™ã¨è§£æã—ã¦DBã«ä¿å­˜ã™ã‚‹ã€‚
å‹•ç”»ã‚’èª¿æŸ»ã—ãŸã¤ã„ã§ã«ä½¿ã†æƒ³å®šã€‚

ä½¿ã„æ–¹:
  python3 tools/youtube_db.py <YouTube URL> --channel-id <channel_id>

ä¾‹:
  python3 tools/youtube_db.py https://www.youtube.com/watch?v=xxxxx --channel-id quizknock-main
  python3 tools/youtube_db.py https://www.youtube.com/watch?v=xxxxx --channel-id sugai-shunki

channel_idã¯data/youtube/channels.jsonã®"id"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒ‡å®šã€‚
çœç•¥ã—ãŸå ´åˆã¯URLã®ã¿ä¿å­˜ï¼ˆchannel_id: unknownï¼‰ã€‚
"""

import sys
import os
import json
import re
import argparse
from datetime import datetime

# tools/ ã¨åŒã˜å ´æ‰€ã«ã‚ã‚‹youtube_analyze.pyã‚’ä½¿ã†
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from youtube_analyze import analyze_youtube

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "data", "youtube")
TRANSCRIPTS_DIR = os.path.join(DATA_DIR, "transcripts")
VIDEOS_JSONL = os.path.join(DATA_DIR, "videos.jsonl")


def extract_video_id(url: str) -> str:
    """YouTube URLã‹ã‚‰video_idã‚’æŠ½å‡º"""
    m = re.search(r'(?:v=|youtu\.be/)([A-Za-z0-9_\-]{11})', url)
    if m:
        return m.group(1)
    raise ValueError(f"video_idãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")


def is_already_analyzed(video_id: str) -> bool:
    """ã™ã§ã«è§£ææ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
    path = os.path.join(TRANSCRIPTS_DIR, f"{video_id}.json")
    return os.path.exists(path)


def load_channels() -> dict:
    """channels.jsonã‚’èª­ã¿è¾¼ã‚€"""
    path = os.path.join(DATA_DIR, "channels.json")
    with open(path) as f:
        data = json.load(f)
    return {c["id"]: c for c in data.get("channels", [])}


def parse_transcript(raw_text: str) -> list:
    """
    Geminiã®ç”Ÿå‡ºåŠ›ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ†ãƒ­ãƒƒãƒ—ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    å½¢å¼: 00:00 ãƒ†ã‚­ã‚¹ãƒˆ
    """
    lines = raw_text.split("\n")
    transcript = []
    for line in lines:
        m = re.match(r'^(\d{1,2}:\d{2})\s+(.+)', line.strip())
        if m:
            transcript.append({"time": m.group(1), "text": m.group(2)})
    return transcript


def extract_title_from_raw(raw_text: str) -> str:
    """ç”Ÿå‡ºåŠ›ã‹ã‚‰å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰"""
    m = re.search(r'å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«[^\n]*\n+[*#\-\s]*(.+)', raw_text)
    if m:
        return m.group(1).strip().lstrip("ã€Œã€")
    return ""


def extract_performers_from_raw(raw_text: str) -> list:
    """ç”Ÿå‡ºåŠ›ã‹ã‚‰å‡ºæ¼”è€…ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰"""
    m = re.search(r'å‡ºæ¼”è€…[^\n]*\n+([\s\S]+?)(?:\n\n|\d\.|###)', raw_text)
    if not m:
        return []
    block = m.group(1)
    performers = []
    for line in block.split("\n"):
        line = line.strip().lstrip("*-ãƒ»").strip()
        if line and len(line) < 30:
            performers.append(line)
    return performers


def save_to_db(video_id: str, channel_id: str, raw_text: str, model: str, url: str):
    """è§£æçµæœã‚’DBã«ä¿å­˜"""
    os.makedirs(TRANSCRIPTS_DIR, exist_ok=True)

    today = datetime.now().strftime("%Y-%m-%d")
    transcript = parse_transcript(raw_text)
    title = extract_title_from_raw(raw_text)
    performers = extract_performers_from_raw(raw_text)

    # transcripts/{video_id}.json
    transcript_data = {
        "video_id": video_id,
        "channel_id": channel_id,
        "url": url,
        "title": title,
        "performers": performers,
        "transcript": transcript,
        "raw": raw_text,
        "analyzed_at": today,
        "model": model
    }
    transcript_path = os.path.join(TRANSCRIPTS_DIR, f"{video_id}.json")
    with open(transcript_path, "w", encoding="utf-8") as f:
        json.dump(transcript_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… Transcript saved: {transcript_path}")

    # videos.jsonl ã«è¿½è¨˜ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    existing_ids = set()
    if os.path.exists(VIDEOS_JSONL):
        with open(VIDEOS_JSONL) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    try:
                        existing_ids.add(json.loads(line)["video_id"])
                    except Exception:
                        pass

    if video_id not in existing_ids:
        video_entry = {
            "video_id": video_id,
            "channel_id": channel_id,
            "url": url,
            "title": title,
            "analyzed_at": today,
            "has_transcript": len(transcript) > 0
        }
        with open(VIDEOS_JSONL, "a", encoding="utf-8") as f:
            f.write(json.dumps(video_entry, ensure_ascii=False) + "\n")
        print(f"âœ… videos.jsonl updated")
    else:
        print(f"â„¹ï¸  videos.jsonl: ã™ã§ã«ç™»éŒ²æ¸ˆã¿ (video_id: {video_id})")


def main():
    parser = argparse.ArgumentParser(description="YouTubeå‹•ç”»ã‚’è§£æã—ã¦DBã«ä¿å­˜")
    parser.add_argument("url", help="YouTubeå‹•ç”»ã®URL")
    parser.add_argument("--channel-id", default="unknown", help="channels.jsonã®id (çœç•¥å¯)")
    parser.add_argument("--model", default="gemini-3.1-pro-preview", help="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--mode", choices=["full", "transcript", "summary"], default="full")
    parser.add_argument("--force", action="store_true", help="è§£ææ¸ˆã¿ã§ã‚‚ä¸Šæ›¸ã")
    args = parser.parse_args()

    # URLã®tãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤ã„ãŸã‚¯ãƒªãƒ¼ãƒ³ãªURLã‚’ä½œã‚‹
    url_clean = re.sub(r'&?t=\d+s?', '', args.url).rstrip('?')

    video_id = extract_video_id(url_clean)
    print(f"ğŸ¬ video_id: {video_id}")
    print(f"ğŸ“ channel_id: {args.channel_id}")

    if is_already_analyzed(video_id) and not args.force:
        print(f"â­ï¸  ã™ã§ã«è§£ææ¸ˆã¿ã§ã™ã€‚ä¸Šæ›¸ãã™ã‚‹ã«ã¯ --force ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"ğŸ” è§£æä¸­... (model: {args.model}, mode: {args.mode})")
    raw = analyze_youtube(url_clean, mode=args.mode, model=args.model)

    save_to_db(video_id, args.channel_id, raw, args.model, url_clean)
    print("\nğŸ“ è§£æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­500æ–‡å­—ï¼‰:")
    print(raw[:500])


if __name__ == "__main__":
    main()
